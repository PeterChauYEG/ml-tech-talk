{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,217\n",
      "Trainable params: 137,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 670,480\n",
      "Trainable params: 670,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.724492, acc: 48.44%] [G loss: 0.936705, mse: 0.937002]\n",
      "1 [D loss: 0.697582, acc: 53.12%] [G loss: 0.922075, mse: 0.922343]\n",
      "2 [D loss: 0.670926, acc: 64.06%] [G loss: 0.904232, mse: 0.904397]\n",
      "3 [D loss: 0.578229, acc: 87.50%] [G loss: 0.861805, mse: 0.861636]\n",
      "4 [D loss: 0.444770, acc: 81.25%] [G loss: 0.782822, mse: 0.781820]\n",
      "5 [D loss: 0.342021, acc: 87.50%] [G loss: 0.655507, mse: 0.653029]\n",
      "6 [D loss: 0.306262, acc: 89.06%] [G loss: 0.538684, mse: 0.534443]\n",
      "7 [D loss: 0.311997, acc: 84.38%] [G loss: 0.444023, mse: 0.438603]\n",
      "8 [D loss: 0.269476, acc: 93.75%] [G loss: 0.351794, mse: 0.345360]\n",
      "9 [D loss: 0.250634, acc: 93.75%] [G loss: 0.303042, mse: 0.296414]\n",
      "10 [D loss: 0.258817, acc: 92.19%] [G loss: 0.300123, mse: 0.293519]\n",
      "11 [D loss: 0.249863, acc: 93.75%] [G loss: 0.266768, mse: 0.260372]\n",
      "12 [D loss: 0.222119, acc: 96.88%] [G loss: 0.292902, mse: 0.287094]\n",
      "13 [D loss: 0.212724, acc: 100.00%] [G loss: 0.249750, mse: 0.243969]\n",
      "14 [D loss: 0.197352, acc: 98.44%] [G loss: 0.266801, mse: 0.261515]\n",
      "15 [D loss: 0.193609, acc: 100.00%] [G loss: 0.286239, mse: 0.281325]\n",
      "16 [D loss: 0.189788, acc: 98.44%] [G loss: 0.278947, mse: 0.274239]\n",
      "17 [D loss: 0.177312, acc: 100.00%] [G loss: 0.264145, mse: 0.259340]\n",
      "18 [D loss: 0.156785, acc: 100.00%] [G loss: 0.275339, mse: 0.270387]\n",
      "19 [D loss: 0.166664, acc: 100.00%] [G loss: 0.294529, mse: 0.289711]\n",
      "20 [D loss: 0.141657, acc: 98.44%] [G loss: 0.258629, mse: 0.253554]\n",
      "21 [D loss: 0.138586, acc: 100.00%] [G loss: 0.271782, mse: 0.266914]\n",
      "22 [D loss: 0.143076, acc: 100.00%] [G loss: 0.277159, mse: 0.272312]\n",
      "23 [D loss: 0.138036, acc: 100.00%] [G loss: 0.269005, mse: 0.263881]\n",
      "24 [D loss: 0.110703, acc: 100.00%] [G loss: 0.272038, mse: 0.266993]\n",
      "25 [D loss: 0.112918, acc: 100.00%] [G loss: 0.245102, mse: 0.239527]\n",
      "26 [D loss: 0.099407, acc: 100.00%] [G loss: 0.255076, mse: 0.249457]\n",
      "27 [D loss: 0.081392, acc: 100.00%] [G loss: 0.262754, mse: 0.257103]\n",
      "28 [D loss: 0.076846, acc: 100.00%] [G loss: 0.291742, mse: 0.286746]\n",
      "29 [D loss: 0.091440, acc: 100.00%] [G loss: 0.265091, mse: 0.259564]\n",
      "30 [D loss: 0.086292, acc: 100.00%] [G loss: 0.254087, mse: 0.248915]\n",
      "31 [D loss: 0.087063, acc: 100.00%] [G loss: 0.263180, mse: 0.257889]\n",
      "32 [D loss: 0.097881, acc: 100.00%] [G loss: 0.254957, mse: 0.249700]\n",
      "33 [D loss: 0.080148, acc: 100.00%] [G loss: 0.225957, mse: 0.220272]\n",
      "34 [D loss: 0.068143, acc: 100.00%] [G loss: 0.254918, mse: 0.248943]\n",
      "35 [D loss: 0.062653, acc: 100.00%] [G loss: 0.243743, mse: 0.237807]\n",
      "36 [D loss: 0.062743, acc: 98.44%] [G loss: 0.254622, mse: 0.249130]\n",
      "37 [D loss: 0.068930, acc: 100.00%] [G loss: 0.241247, mse: 0.236052]\n",
      "38 [D loss: 0.066864, acc: 100.00%] [G loss: 0.238894, mse: 0.232968]\n",
      "39 [D loss: 0.049563, acc: 100.00%] [G loss: 0.247957, mse: 0.242024]\n",
      "40 [D loss: 0.069187, acc: 100.00%] [G loss: 0.227469, mse: 0.221273]\n",
      "41 [D loss: 0.059830, acc: 100.00%] [G loss: 0.239390, mse: 0.233516]\n",
      "42 [D loss: 0.068616, acc: 100.00%] [G loss: 0.231738, mse: 0.225388]\n",
      "43 [D loss: 0.060266, acc: 100.00%] [G loss: 0.232754, mse: 0.225322]\n",
      "44 [D loss: 0.043019, acc: 100.00%] [G loss: 0.233532, mse: 0.227152]\n",
      "45 [D loss: 0.061221, acc: 98.44%] [G loss: 0.222131, mse: 0.214791]\n",
      "46 [D loss: 0.057173, acc: 100.00%] [G loss: 0.219730, mse: 0.212581]\n",
      "47 [D loss: 0.040597, acc: 100.00%] [G loss: 0.213854, mse: 0.205908]\n",
      "48 [D loss: 0.054724, acc: 100.00%] [G loss: 0.220674, mse: 0.213824]\n",
      "49 [D loss: 0.106938, acc: 95.31%] [G loss: 0.216351, mse: 0.210373]\n",
      "50 [D loss: 0.082127, acc: 96.88%] [G loss: 0.227256, mse: 0.219442]\n",
      "51 [D loss: 0.067467, acc: 98.44%] [G loss: 0.231154, mse: 0.223218]\n",
      "52 [D loss: 0.041434, acc: 100.00%] [G loss: 0.220138, mse: 0.213083]\n",
      "53 [D loss: 0.056884, acc: 100.00%] [G loss: 0.210190, mse: 0.201113]\n",
      "54 [D loss: 0.069418, acc: 98.44%] [G loss: 0.203510, mse: 0.194825]\n",
      "55 [D loss: 0.069631, acc: 100.00%] [G loss: 0.194353, mse: 0.185553]\n",
      "56 [D loss: 0.109207, acc: 96.88%] [G loss: 0.229642, mse: 0.221530]\n",
      "57 [D loss: 0.078500, acc: 96.88%] [G loss: 0.202479, mse: 0.194721]\n",
      "58 [D loss: 0.054429, acc: 100.00%] [G loss: 0.208907, mse: 0.200883]\n",
      "59 [D loss: 0.067792, acc: 100.00%] [G loss: 0.200728, mse: 0.193112]\n",
      "60 [D loss: 0.097269, acc: 96.88%] [G loss: 0.189016, mse: 0.181011]\n",
      "61 [D loss: 0.092078, acc: 98.44%] [G loss: 0.213531, mse: 0.206330]\n",
      "62 [D loss: 0.066572, acc: 98.44%] [G loss: 0.185443, mse: 0.175749]\n",
      "63 [D loss: 0.047857, acc: 100.00%] [G loss: 0.184440, mse: 0.175844]\n",
      "64 [D loss: 0.142408, acc: 92.19%] [G loss: 0.214435, mse: 0.207107]\n",
      "65 [D loss: 0.171620, acc: 96.88%] [G loss: 0.203152, mse: 0.195378]\n",
      "66 [D loss: 0.091069, acc: 95.31%] [G loss: 0.204063, mse: 0.193707]\n",
      "67 [D loss: 0.121955, acc: 95.31%] [G loss: 0.197210, mse: 0.189870]\n",
      "68 [D loss: 0.249783, acc: 90.62%] [G loss: 0.211443, mse: 0.203481]\n",
      "69 [D loss: 0.111788, acc: 96.88%] [G loss: 0.212254, mse: 0.203930]\n",
      "70 [D loss: 0.222514, acc: 87.50%] [G loss: 0.198545, mse: 0.190775]\n",
      "71 [D loss: 0.135502, acc: 90.62%] [G loss: 0.192316, mse: 0.177077]\n",
      "72 [D loss: 0.114925, acc: 95.31%] [G loss: 0.171075, mse: 0.156175]\n",
      "73 [D loss: 0.096545, acc: 96.88%] [G loss: 0.198000, mse: 0.185757]\n",
      "74 [D loss: 0.189521, acc: 90.62%] [G loss: 0.193293, mse: 0.184156]\n",
      "75 [D loss: 0.136315, acc: 92.19%] [G loss: 0.190203, mse: 0.181071]\n",
      "76 [D loss: 0.118892, acc: 95.31%] [G loss: 0.160294, mse: 0.153009]\n",
      "77 [D loss: 0.178224, acc: 92.19%] [G loss: 0.183314, mse: 0.176854]\n",
      "78 [D loss: 0.094055, acc: 95.31%] [G loss: 0.174784, mse: 0.166785]\n",
      "79 [D loss: 0.265331, acc: 90.62%] [G loss: 0.186035, mse: 0.178840]\n",
      "80 [D loss: 0.279049, acc: 84.38%] [G loss: 0.170671, mse: 0.162592]\n",
      "81 [D loss: 0.217741, acc: 87.50%] [G loss: 0.173247, mse: 0.165623]\n",
      "82 [D loss: 0.143064, acc: 93.75%] [G loss: 0.188142, mse: 0.179467]\n",
      "83 [D loss: 0.217875, acc: 89.06%] [G loss: 0.184327, mse: 0.178161]\n",
      "84 [D loss: 0.181473, acc: 90.62%] [G loss: 0.186049, mse: 0.172887]\n",
      "85 [D loss: 0.353745, acc: 85.94%] [G loss: 0.178536, mse: 0.170323]\n",
      "86 [D loss: 0.271141, acc: 85.94%] [G loss: 0.183822, mse: 0.174699]\n",
      "87 [D loss: 0.407234, acc: 87.50%] [G loss: 0.178035, mse: 0.171584]\n",
      "88 [D loss: 0.243066, acc: 84.38%] [G loss: 0.171807, mse: 0.161805]\n",
      "89 [D loss: 0.428369, acc: 79.69%] [G loss: 0.161914, mse: 0.155440]\n",
      "90 [D loss: 0.142471, acc: 93.75%] [G loss: 0.168496, mse: 0.155593]\n",
      "91 [D loss: 0.274053, acc: 87.50%] [G loss: 0.161745, mse: 0.153642]\n",
      "92 [D loss: 0.427011, acc: 84.38%] [G loss: 0.177054, mse: 0.171214]\n",
      "93 [D loss: 0.133347, acc: 95.31%] [G loss: 0.184752, mse: 0.171627]\n",
      "94 [D loss: 0.440645, acc: 78.12%] [G loss: 0.159532, mse: 0.151190]\n",
      "95 [D loss: 0.462130, acc: 71.88%] [G loss: 0.160631, mse: 0.155683]\n",
      "96 [D loss: 0.308891, acc: 84.38%] [G loss: 0.180481, mse: 0.165900]\n",
      "97 [D loss: 0.390732, acc: 79.69%] [G loss: 0.180515, mse: 0.167520]\n",
      "98 [D loss: 0.347114, acc: 82.81%] [G loss: 0.154460, mse: 0.146693]\n",
      "99 [D loss: 0.283267, acc: 87.50%] [G loss: 0.147369, mse: 0.137672]\n",
      "100 [D loss: 0.376142, acc: 87.50%] [G loss: 0.179370, mse: 0.172217]\n",
      "101 [D loss: 0.245096, acc: 87.50%] [G loss: 0.175480, mse: 0.167872]\n",
      "102 [D loss: 0.149792, acc: 95.31%] [G loss: 0.165795, mse: 0.156266]\n",
      "103 [D loss: 0.335863, acc: 85.94%] [G loss: 0.171041, mse: 0.166289]\n",
      "104 [D loss: 0.241070, acc: 87.50%] [G loss: 0.166850, mse: 0.159488]\n",
      "105 [D loss: 0.331991, acc: 79.69%] [G loss: 0.153379, mse: 0.146862]\n",
      "106 [D loss: 0.257172, acc: 87.50%] [G loss: 0.157852, mse: 0.152189]\n",
      "107 [D loss: 0.340671, acc: 81.25%] [G loss: 0.154096, mse: 0.148355]\n",
      "108 [D loss: 0.319872, acc: 81.25%] [G loss: 0.163864, mse: 0.159397]\n",
      "109 [D loss: 0.208417, acc: 89.06%] [G loss: 0.186290, mse: 0.174265]\n",
      "110 [D loss: 0.228736, acc: 87.50%] [G loss: 0.159088, mse: 0.153644]\n",
      "111 [D loss: 0.450604, acc: 78.12%] [G loss: 0.146920, mse: 0.141807]\n",
      "112 [D loss: 0.299969, acc: 79.69%] [G loss: 0.155304, mse: 0.144346]\n",
      "113 [D loss: 0.298456, acc: 84.38%] [G loss: 0.175863, mse: 0.169080]\n",
      "114 [D loss: 0.429252, acc: 79.69%] [G loss: 0.174042, mse: 0.167441]\n",
      "115 [D loss: 0.291188, acc: 82.81%] [G loss: 0.159504, mse: 0.149200]\n",
      "116 [D loss: 0.272964, acc: 85.94%] [G loss: 0.161036, mse: 0.154628]\n",
      "117 [D loss: 0.372193, acc: 82.81%] [G loss: 0.157048, mse: 0.150974]\n",
      "118 [D loss: 0.200603, acc: 92.19%] [G loss: 0.148898, mse: 0.139567]\n",
      "119 [D loss: 0.256411, acc: 84.38%] [G loss: 0.160057, mse: 0.153423]\n",
      "120 [D loss: 0.304965, acc: 85.94%] [G loss: 0.163380, mse: 0.157431]\n",
      "121 [D loss: 0.273493, acc: 84.38%] [G loss: 0.157050, mse: 0.148640]\n",
      "122 [D loss: 0.274535, acc: 82.81%] [G loss: 0.172146, mse: 0.166382]\n",
      "123 [D loss: 0.284487, acc: 87.50%] [G loss: 0.131197, mse: 0.124800]\n",
      "124 [D loss: 0.236190, acc: 90.62%] [G loss: 0.155763, mse: 0.149241]\n",
      "125 [D loss: 0.238224, acc: 89.06%] [G loss: 0.131296, mse: 0.124608]\n",
      "126 [D loss: 0.299486, acc: 84.38%] [G loss: 0.150205, mse: 0.145473]\n",
      "127 [D loss: 0.221780, acc: 89.06%] [G loss: 0.153845, mse: 0.146493]\n",
      "128 [D loss: 0.366470, acc: 82.81%] [G loss: 0.173292, mse: 0.168944]\n",
      "129 [D loss: 0.188729, acc: 90.62%] [G loss: 0.165234, mse: 0.154839]\n",
      "130 [D loss: 0.230219, acc: 92.19%] [G loss: 0.145798, mse: 0.139980]\n",
      "131 [D loss: 0.168216, acc: 93.75%] [G loss: 0.147804, mse: 0.141914]\n",
      "132 [D loss: 0.219279, acc: 89.06%] [G loss: 0.138768, mse: 0.132519]\n",
      "133 [D loss: 0.196644, acc: 92.19%] [G loss: 0.142140, mse: 0.137127]\n",
      "134 [D loss: 0.165551, acc: 93.75%] [G loss: 0.142510, mse: 0.135930]\n",
      "135 [D loss: 0.326683, acc: 89.06%] [G loss: 0.163948, mse: 0.158980]\n",
      "136 [D loss: 0.192299, acc: 93.75%] [G loss: 0.156657, mse: 0.149458]\n",
      "137 [D loss: 0.216454, acc: 93.75%] [G loss: 0.153440, mse: 0.147033]\n",
      "138 [D loss: 0.220193, acc: 89.06%] [G loss: 0.150592, mse: 0.143240]\n",
      "139 [D loss: 0.313167, acc: 81.25%] [G loss: 0.161994, mse: 0.156266]\n",
      "140 [D loss: 0.220674, acc: 93.75%] [G loss: 0.144412, mse: 0.137398]\n",
      "141 [D loss: 0.233247, acc: 92.19%] [G loss: 0.141329, mse: 0.135836]\n",
      "142 [D loss: 0.245368, acc: 89.06%] [G loss: 0.149275, mse: 0.142560]\n",
      "143 [D loss: 0.265970, acc: 89.06%] [G loss: 0.157045, mse: 0.151778]\n",
      "144 [D loss: 0.284036, acc: 87.50%] [G loss: 0.153575, mse: 0.147390]\n",
      "145 [D loss: 0.233171, acc: 87.50%] [G loss: 0.132682, mse: 0.125848]\n",
      "146 [D loss: 0.190130, acc: 95.31%] [G loss: 0.131017, mse: 0.125307]\n",
      "147 [D loss: 0.209823, acc: 93.75%] [G loss: 0.135367, mse: 0.130429]\n",
      "148 [D loss: 0.199382, acc: 92.19%] [G loss: 0.162599, mse: 0.157133]\n",
      "149 [D loss: 0.256847, acc: 92.19%] [G loss: 0.151320, mse: 0.146103]\n",
      "150 [D loss: 0.155316, acc: 98.44%] [G loss: 0.146796, mse: 0.140661]\n",
      "151 [D loss: 0.156528, acc: 100.00%] [G loss: 0.143769, mse: 0.138575]\n",
      "152 [D loss: 0.178945, acc: 95.31%] [G loss: 0.135755, mse: 0.131301]\n",
      "153 [D loss: 0.169294, acc: 96.88%] [G loss: 0.156932, mse: 0.150744]\n",
      "154 [D loss: 0.207946, acc: 95.31%] [G loss: 0.142083, mse: 0.137559]\n",
      "155 [D loss: 0.161529, acc: 93.75%] [G loss: 0.153863, mse: 0.147490]\n",
      "156 [D loss: 0.168815, acc: 95.31%] [G loss: 0.126713, mse: 0.122122]\n",
      "157 [D loss: 0.214498, acc: 95.31%] [G loss: 0.148691, mse: 0.143558]\n",
      "158 [D loss: 0.148173, acc: 98.44%] [G loss: 0.128023, mse: 0.122707]\n",
      "159 [D loss: 0.173213, acc: 96.88%] [G loss: 0.139525, mse: 0.134774]\n",
      "160 [D loss: 0.192392, acc: 93.75%] [G loss: 0.122544, mse: 0.117070]\n",
      "161 [D loss: 0.174564, acc: 96.88%] [G loss: 0.150510, mse: 0.145884]\n",
      "162 [D loss: 0.172699, acc: 95.31%] [G loss: 0.146473, mse: 0.142045]\n",
      "163 [D loss: 0.151709, acc: 96.88%] [G loss: 0.154304, mse: 0.148236]\n",
      "164 [D loss: 0.167574, acc: 96.88%] [G loss: 0.132351, mse: 0.127670]\n",
      "165 [D loss: 0.149311, acc: 98.44%] [G loss: 0.158114, mse: 0.153478]\n",
      "166 [D loss: 0.124826, acc: 98.44%] [G loss: 0.138736, mse: 0.133435]\n",
      "167 [D loss: 0.222642, acc: 96.88%] [G loss: 0.127019, mse: 0.122502]\n",
      "168 [D loss: 0.130349, acc: 100.00%] [G loss: 0.115338, mse: 0.109821]\n",
      "169 [D loss: 0.144229, acc: 96.88%] [G loss: 0.134699, mse: 0.128870]\n",
      "170 [D loss: 0.227522, acc: 95.31%] [G loss: 0.144175, mse: 0.139628]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-63d6fba9fb15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0maae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdversarialAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0maae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-63d6fba9fb15>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversarial_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdversarialAutoencoder():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 10\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the encoder / decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        # The generator takes the image, encodes it and reconstructs it\n",
    "        # from the encoding\n",
    "        encoded_repr = self.encoder(img)\n",
    "        reconstructed_img = self.decoder(encoded_repr)\n",
    "\n",
    "        # For the adversarial_autoencoder model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator determines validity of the encoding\n",
    "        validity = self.discriminator(encoded_repr)\n",
    "\n",
    "        # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "        self.adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "        self.adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'],\n",
    "            loss_weights=[0.999, 0.001],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        # Encoder\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        h = Flatten()(img)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        mu = Dense(self.latent_dim)(h)\n",
    "        log_var = Dense(self.latent_dim)(h)\n",
    "        latent_repr = Lambda(\n",
    "                lambda p: p[0] + K.random_normal(K.shape(p[0])) * K.exp(p[1] / 2),\n",
    "                output_shape=lambda p: p[0]\n",
    "        )([mu, log_var])\n",
    "\n",
    "        return Model(img, latent_repr)\n",
    "\n",
    "    def build_decoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = model(z)\n",
    "\n",
    "        return Model(z, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.summary()\n",
    "\n",
    "        encoded_repr = Input(shape=(self.latent_dim, ))\n",
    "        validity = model(encoded_repr)\n",
    "\n",
    "        return Model(encoded_repr, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            latent_fake = self.encoder.predict(imgs)\n",
    "            latent_real = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "\n",
    "        z = np.random.normal(size=(r*c, self.latent_dim))\n",
    "        gen_imgs = self.decoder.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.decoder, \"aae_generator\")\n",
    "        save(self.encoder, \"aae_discriminator\")\n",
    "        self.decoder.save('saved_model/aae_generator.h5')\n",
    "        self.encoder.save('saved_model/aae_discriminator.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aae = AdversarialAutoencoder()\n",
    "    aae.train(epochs=20000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
