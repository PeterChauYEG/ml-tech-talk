{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,217\n",
      "Trainable params: 137,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 670,480\n",
      "Trainable params: 670,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.747474, acc: 26.56%] [G loss: 0.947737, mse: 0.948034]\n",
      "200 [D loss: 0.125337, acc: 98.44%] [G loss: 0.136198, mse: 0.130434]\n",
      "400 [D loss: 0.025628, acc: 100.00%] [G loss: 0.124885, mse: 0.118206]\n",
      "600 [D loss: 0.022782, acc: 98.44%] [G loss: 0.110968, mse: 0.102987]\n",
      "800 [D loss: 0.012115, acc: 100.00%] [G loss: 0.103591, mse: 0.095675]\n",
      "1000 [D loss: 0.006177, acc: 100.00%] [G loss: 0.116539, mse: 0.106106]\n",
      "1200 [D loss: 0.054681, acc: 98.44%] [G loss: 0.103271, mse: 0.094721]\n",
      "1400 [D loss: 0.038381, acc: 98.44%] [G loss: 0.126682, mse: 0.118335]\n",
      "1600 [D loss: 0.102418, acc: 95.31%] [G loss: 0.107659, mse: 0.099100]\n",
      "1800 [D loss: 0.041890, acc: 96.88%] [G loss: 0.116632, mse: 0.108199]\n",
      "2000 [D loss: 0.160657, acc: 96.88%] [G loss: 0.098449, mse: 0.091664]\n",
      "2200 [D loss: 0.128247, acc: 93.75%] [G loss: 0.109436, mse: 0.101621]\n",
      "2400 [D loss: 0.090413, acc: 98.44%] [G loss: 0.107764, mse: 0.100651]\n",
      "2600 [D loss: 0.187468, acc: 95.31%] [G loss: 0.095220, mse: 0.089902]\n",
      "2800 [D loss: 0.086763, acc: 96.88%] [G loss: 0.093533, mse: 0.088149]\n",
      "3000 [D loss: 0.122671, acc: 93.75%] [G loss: 0.097785, mse: 0.091517]\n",
      "3200 [D loss: 0.194682, acc: 87.50%] [G loss: 0.093625, mse: 0.088975]\n",
      "3400 [D loss: 0.170800, acc: 95.31%] [G loss: 0.099488, mse: 0.093537]\n",
      "3600 [D loss: 0.084735, acc: 95.31%] [G loss: 0.089383, mse: 0.084409]\n",
      "3800 [D loss: 0.123832, acc: 98.44%] [G loss: 0.083155, mse: 0.078288]\n",
      "4000 [D loss: 0.112932, acc: 92.19%] [G loss: 0.099193, mse: 0.094297]\n",
      "4200 [D loss: 0.152530, acc: 95.31%] [G loss: 0.103339, mse: 0.097552]\n",
      "4400 [D loss: 0.383088, acc: 79.69%] [G loss: 0.087687, mse: 0.083836]\n",
      "4600 [D loss: 0.227429, acc: 90.62%] [G loss: 0.099327, mse: 0.093278]\n",
      "4800 [D loss: 0.153036, acc: 92.19%] [G loss: 0.093948, mse: 0.088345]\n",
      "5000 [D loss: 0.162064, acc: 95.31%] [G loss: 0.072070, mse: 0.067502]\n",
      "5200 [D loss: 0.194776, acc: 93.75%] [G loss: 0.092549, mse: 0.087653]\n",
      "5400 [D loss: 0.212646, acc: 95.31%] [G loss: 0.084117, mse: 0.080247]\n",
      "5600 [D loss: 0.157297, acc: 96.88%] [G loss: 0.088857, mse: 0.082857]\n",
      "5800 [D loss: 0.144208, acc: 96.88%] [G loss: 0.095406, mse: 0.091274]\n",
      "6000 [D loss: 0.093165, acc: 98.44%] [G loss: 0.089640, mse: 0.084398]\n",
      "6200 [D loss: 0.175720, acc: 93.75%] [G loss: 0.087334, mse: 0.082380]\n",
      "6400 [D loss: 0.268538, acc: 87.50%] [G loss: 0.084029, mse: 0.080343]\n",
      "6600 [D loss: 0.102922, acc: 98.44%] [G loss: 0.078333, mse: 0.073665]\n",
      "6800 [D loss: 0.223162, acc: 89.06%] [G loss: 0.104562, mse: 0.100471]\n",
      "7000 [D loss: 0.250187, acc: 89.06%] [G loss: 0.120162, mse: 0.116305]\n",
      "7200 [D loss: 0.183699, acc: 93.75%] [G loss: 0.098550, mse: 0.093438]\n",
      "7400 [D loss: 0.237116, acc: 87.50%] [G loss: 0.077215, mse: 0.072416]\n",
      "7600 [D loss: 0.268718, acc: 90.62%] [G loss: 0.095147, mse: 0.090131]\n",
      "7800 [D loss: 0.277157, acc: 85.94%] [G loss: 0.078578, mse: 0.074452]\n",
      "8000 [D loss: 0.172486, acc: 92.19%] [G loss: 0.087433, mse: 0.082620]\n",
      "8200 [D loss: 0.319938, acc: 87.50%] [G loss: 0.095007, mse: 0.090996]\n",
      "8400 [D loss: 0.119025, acc: 96.88%] [G loss: 0.085045, mse: 0.081351]\n",
      "8600 [D loss: 0.233235, acc: 89.06%] [G loss: 0.104486, mse: 0.100493]\n",
      "8800 [D loss: 0.302152, acc: 85.94%] [G loss: 0.082770, mse: 0.078180]\n",
      "9000 [D loss: 0.238115, acc: 90.62%] [G loss: 0.084590, mse: 0.081121]\n",
      "9200 [D loss: 0.334591, acc: 81.25%] [G loss: 0.101101, mse: 0.096466]\n",
      "9400 [D loss: 0.317678, acc: 85.94%] [G loss: 0.095814, mse: 0.092334]\n",
      "9600 [D loss: 0.259982, acc: 89.06%] [G loss: 0.087026, mse: 0.083699]\n",
      "9800 [D loss: 0.289253, acc: 89.06%] [G loss: 0.090509, mse: 0.086947]\n",
      "10000 [D loss: 0.154252, acc: 96.88%] [G loss: 0.088070, mse: 0.084590]\n",
      "10200 [D loss: 0.297234, acc: 90.62%] [G loss: 0.088272, mse: 0.084272]\n",
      "10400 [D loss: 0.276172, acc: 81.25%] [G loss: 0.086635, mse: 0.082549]\n",
      "10600 [D loss: 0.341153, acc: 84.38%] [G loss: 0.093995, mse: 0.089452]\n",
      "10800 [D loss: 0.180961, acc: 93.75%] [G loss: 0.090404, mse: 0.087185]\n",
      "11000 [D loss: 0.299782, acc: 87.50%] [G loss: 0.101252, mse: 0.097528]\n",
      "11200 [D loss: 0.417406, acc: 79.69%] [G loss: 0.087057, mse: 0.084407]\n",
      "11400 [D loss: 0.236239, acc: 92.19%] [G loss: 0.082210, mse: 0.078513]\n",
      "11600 [D loss: 0.209587, acc: 89.06%] [G loss: 0.072914, mse: 0.069508]\n",
      "11800 [D loss: 0.357692, acc: 92.19%] [G loss: 0.071905, mse: 0.068666]\n",
      "12000 [D loss: 0.276212, acc: 90.62%] [G loss: 0.074561, mse: 0.071591]\n",
      "12200 [D loss: 0.222825, acc: 96.88%] [G loss: 0.084809, mse: 0.081174]\n",
      "12400 [D loss: 0.219482, acc: 90.62%] [G loss: 0.073269, mse: 0.069256]\n",
      "12600 [D loss: 0.281330, acc: 87.50%] [G loss: 0.083103, mse: 0.079716]\n",
      "12800 [D loss: 0.414966, acc: 81.25%] [G loss: 0.077046, mse: 0.074579]\n",
      "13000 [D loss: 0.273344, acc: 89.06%] [G loss: 0.088357, mse: 0.084686]\n",
      "13200 [D loss: 0.342242, acc: 87.50%] [G loss: 0.082970, mse: 0.079952]\n",
      "13400 [D loss: 0.258191, acc: 89.06%] [G loss: 0.080986, mse: 0.077628]\n",
      "13600 [D loss: 0.223224, acc: 89.06%] [G loss: 0.090539, mse: 0.087267]\n",
      "13800 [D loss: 0.232967, acc: 89.06%] [G loss: 0.082013, mse: 0.078194]\n",
      "14000 [D loss: 0.243739, acc: 89.06%] [G loss: 0.074825, mse: 0.071435]\n",
      "14200 [D loss: 0.271891, acc: 85.94%] [G loss: 0.092138, mse: 0.088133]\n",
      "14400 [D loss: 0.284422, acc: 87.50%] [G loss: 0.071220, mse: 0.068445]\n",
      "14600 [D loss: 0.326442, acc: 85.94%] [G loss: 0.081016, mse: 0.077964]\n",
      "14800 [D loss: 0.329272, acc: 87.50%] [G loss: 0.077223, mse: 0.074449]\n",
      "15000 [D loss: 0.249043, acc: 89.06%] [G loss: 0.081409, mse: 0.077979]\n",
      "15200 [D loss: 0.402085, acc: 84.38%] [G loss: 0.075213, mse: 0.071686]\n",
      "15400 [D loss: 0.227420, acc: 90.62%] [G loss: 0.079698, mse: 0.076140]\n",
      "15600 [D loss: 0.508144, acc: 78.12%] [G loss: 0.088009, mse: 0.085690]\n",
      "15800 [D loss: 0.308378, acc: 87.50%] [G loss: 0.071544, mse: 0.068844]\n",
      "16000 [D loss: 0.343546, acc: 81.25%] [G loss: 0.086067, mse: 0.083394]\n",
      "16200 [D loss: 0.324807, acc: 84.38%] [G loss: 0.076640, mse: 0.073925]\n",
      "16400 [D loss: 0.319314, acc: 84.38%] [G loss: 0.072624, mse: 0.069874]\n",
      "16600 [D loss: 0.305830, acc: 84.38%] [G loss: 0.074293, mse: 0.071467]\n",
      "16800 [D loss: 0.316246, acc: 89.06%] [G loss: 0.074833, mse: 0.071827]\n",
      "17000 [D loss: 0.186128, acc: 93.75%] [G loss: 0.074718, mse: 0.070918]\n",
      "17200 [D loss: 0.300384, acc: 85.94%] [G loss: 0.072079, mse: 0.069138]\n",
      "17400 [D loss: 0.285891, acc: 87.50%] [G loss: 0.070114, mse: 0.067642]\n",
      "17600 [D loss: 0.255600, acc: 89.06%] [G loss: 0.071011, mse: 0.067855]\n",
      "17800 [D loss: 0.314408, acc: 82.81%] [G loss: 0.075470, mse: 0.072128]\n",
      "18000 [D loss: 0.276982, acc: 82.81%] [G loss: 0.083511, mse: 0.080291]\n",
      "18200 [D loss: 0.373256, acc: 82.81%] [G loss: 0.082088, mse: 0.079494]\n",
      "18400 [D loss: 0.260169, acc: 87.50%] [G loss: 0.065986, mse: 0.062414]\n",
      "18600 [D loss: 0.339350, acc: 90.62%] [G loss: 0.082786, mse: 0.079569]\n",
      "18800 [D loss: 0.254825, acc: 90.62%] [G loss: 0.077704, mse: 0.074410]\n",
      "19000 [D loss: 0.274430, acc: 92.19%] [G loss: 0.073244, mse: 0.070761]\n",
      "19200 [D loss: 0.294653, acc: 93.75%] [G loss: 0.052608, mse: 0.049739]\n",
      "19400 [D loss: 0.336558, acc: 87.50%] [G loss: 0.061470, mse: 0.058860]\n",
      "19600 [D loss: 0.347897, acc: 84.38%] [G loss: 0.067798, mse: 0.065164]\n",
      "19800 [D loss: 0.338569, acc: 82.81%] [G loss: 0.073150, mse: 0.071164]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdversarialAutoencoder():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 10\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the encoder / decoder\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        # The generator takes the image, encodes it and reconstructs it\n",
    "        # from the encoding\n",
    "        encoded_repr = self.encoder(img)\n",
    "        reconstructed_img = self.decoder(encoded_repr)\n",
    "\n",
    "        # For the adversarial_autoencoder model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator determines validity of the encoding\n",
    "        validity = self.discriminator(encoded_repr)\n",
    "\n",
    "        # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "        self.adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "        self.adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'],\n",
    "            loss_weights=[0.999, 0.001],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        # Encoder\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        h = Flatten()(img)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        h = Dense(512)(h)\n",
    "        h = LeakyReLU(alpha=0.2)(h)\n",
    "        mu = Dense(self.latent_dim)(h)\n",
    "        log_var = Dense(self.latent_dim)(h)\n",
    "        latent_repr = Lambda(\n",
    "                lambda p: p[0] + K.random_normal(K.shape(p[0])) * K.exp(p[1] / 2),\n",
    "                output_shape=lambda p: p[0]\n",
    "        )([mu, log_var])\n",
    "\n",
    "        return Model(img, latent_repr)\n",
    "\n",
    "    def build_decoder(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = model(z)\n",
    "\n",
    "        return Model(z, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.summary()\n",
    "\n",
    "        encoded_repr = Input(shape=(self.latent_dim, ))\n",
    "        validity = model(encoded_repr)\n",
    "\n",
    "        return Model(encoded_repr, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            latent_fake = self.encoder.predict(imgs)\n",
    "            latent_real = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "\n",
    "        z = np.random.normal(size=(r*c, self.latent_dim))\n",
    "        gen_imgs = self.decoder.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.decoder, \"aae_generator\")\n",
    "        save(self.encoder, \"aae_discriminator\")\n",
    "        self.decoder.save('saved_model/aae_generator.h5')\n",
    "        self.encoder.save('saved_model/aae_discriminator.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aae = AdversarialAutoencoder()\n",
    "    aae.train(epochs=20000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
